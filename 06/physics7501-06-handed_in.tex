\documentclass[11pt, english, fleqn, DIV=15, headinclude]{scrartcl}

\usepackage[bibatend]{../header}
\usepackage{../my-boxes}

\usepackage{lastpage}
\usepackage{multicol}
\usepackage{simplewick}
\usepackage{multicol}
\usepackage{slashed}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{tikzsymbols}

\newcommand\timeorder{\mathscr T}
\newcommand\normorder{\mathscr N}
\newcommand\eye{\mat 1_4}
\newcommand\fourslash[1]{\slashed{\four{#1}}}
\newcommand\T{\mathrm T}

\hypersetup{
    pdftitle=
}

\graphicspath{{build/}}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{6}

\subject{physics7501 -- Advanced Quantum Field Theory}
\ihead{physics7501 -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\newcommand\thegroup{Tutor: Thorsten Schimannek}

\publishers{\thegroup}
\ofoot{\thegroup}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
}
\ifoot{Martin Ueding}

\ohead{\rightmark}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        Problem & Achieved points & Possible points \\
        \midrule
        \nameref{homework:1} & & \punkte{15} \\
        \midrule
        Total & & \arabic{totalpoints}
    \end{tabular}
\end{center}

\vspace{3ex}

\begin{center}
    \begin{small}
        This document consists of \pageref{LastPage} pages.
    \end{small}
\end{center}

\section{Grassmann numbers and the fermionic path integral}
\label{homework:1}

\subsection{Taylor series and shift}

The Grassmann numbers themselves are the generators of the algebra. It probably
makes sense to add 1 to the generators also in order to have $\C$  be part of
the algebra as well. We will call that set $\setname G$.\footnote{%
    The notation $\mathbb C$ and $\mathbb G$ might be more common for fields. I like to go
    with the ISO 80000-2 standard (which allows both variants) and also take
    the heritage of those letters (like $\mathbb C$) into account. In \LaTeX\
    they are called “blackboard bold” which means this way of writing letters
    is the attempt to write bold on the blackboard. Since we have more powerful
    typesetting in \LaTeX\ than on the board, I will sure make use of them and
    display the letters in bold. Same goes for underline for emphasis, which is
    a no-go in typed documents. Perhaps it is not the best idea to deviate too
    much from common notation even though its usage in a powerful typesetting
    system feels rather backwards.
}
The function $f$ then is
\[
    f \colon \setname G \to \setname G \,.
\]
Since the Grassmann numbers anticommute they square to zero.\footnote{%
    On the Wikipedia article about Grassmann numbers they are called “non-zero
    square roots of zero” which I find a quite intriguing thought. Almost ten
    years ago the concept of $\sqrt{-1}$ has baffled me, now this is the next
    step. This time I know that field axioms are not god-given and therefore
    algebra structures can be defined like one wishes to furnish those axioms.
    Still, it is an interesting way of stating the squaring to zero feature.
}
This also means that a function of a single Grassmann variable cannot have
arbitrary form. Analytic functions can always be written as a power series,
functions can at most be linear in Grassmann variables which really limits
their complexity. Any function can be written as
\[
    f(\theta) = A + B \theta \,,
\]
where we use the same notation as \textcite[299]{Peskin/QFT/1995}. Higher order
terms in $\theta$ vanish directly.

\paragraph{Taylor series}

The Taylor series of such a function in $\theta$ just has two terms as higher
powers of $\theta$ will vanish. Also higher derivatives in $\theta$ will also
vanish when one looks at the general form motivated above.

\paragraph{Integral}

The integration must be invariant under shifts in the integration variable. We
have
\begin{align*}
    \int \dif \theta \, f(\theta)
    &= \int \dif \theta \, [A + B \theta]
    \intertext{%
        as the general form. Now shifting $\theta$ by $\eta$ does not change
        $\dif \theta$ and we have
    }
    &= \int \dif \theta \, [A + B [\theta + \eta]]
    \intertext{%
        which we can expand to yield
    }
    &= \int \dif \theta \, [A + B \theta + B \eta] \,.
    \intertext{%
        It is best to regroup the terms such that the constant and linear terms
        clearly pop out:
    }
    &= \int \dif \theta \, [\underbrace{A + B \eta}_{A'} + B \theta] \,.
\end{align*}
The result of the integration must not be changed when $A \to A'$ it performed.
Therefore the integral must not depend on $A$ as it is a linear function and
has to be rather simple. The only dependence can be $B$ then. A definition that
fulfils this is
\[
    \int \dif \theta \, [A + B\theta] = k B \,.
\]

Could this $k$ be a Grassmann number, i.e.\ $k \in \setname G$? $B$ is
definitely a regular complex number, so the product would not directly
vanish. However, the integral would then be a $\setname G$-linear function
which does not play well with integrations in $\C$. It therefore makes sense to
define $k \in \C$ and therefore $kB = c$ is a regular complex number.

\paragraph{Integration and Differentiation}

On the problem set it is noted that integration and differentiation are
equivalent. Since there are only two possible monoms of Grassmann variables
($A$ and $B \theta$), this is easy to check. The integral and derivative with
respect to $\theta$ of the first term is zero. The integral and derivative of
the second is just $B$. Setting $k = 1$ makes this truly the same thing which
is probably very handy along the road.

\subsection{Unitary transformation}

The unitary transformation is probably a \emph{special} unitary transformation.
In the case of a general unitary transformation the determinant might be a
complex phase factor which does change the integrals unless complex conjugated
counter-terms are involved as well. So $U$ is taken from $\SU(n)$ and $\mathrm
U(1)$ is excluded here? Since the problem talks about matrices, $\SU(n)$ would
not really be a problem. So this just applies when there are multiple fields?
\Textcite[301]{Peskin/QFT/1995} show that the integral with $\dif \theta^* \dif
\theta$ is invariant under $\mathrm U(n)$ but integrals with $\dif \theta$
depend on $\det(U)$ which might have a phase factor.

We will now show that the transformation introduces a factor on $\det(U)$. The
invariance has to be evaluated with respect of that determinant then which is
unity for the special transformations.

\begin{table}
    \begin{question}
        $\mathrm U(1)$ only has one generator, 1. Therefore the group $\SU(1) =
        \{ 1 \}$ is a quite trivial group. The groups $\SU(n)$ all have $n^2-1$
        generators and $\mathrm U(n)$ should have $n^2$ generators. Is it in
        general that
        \[
            \mathrm U(n) \simeq \mathrm U(1) \times \SU(n)
        \]
        holds?
    \end{question}
\end{table}

First we show a useful identity. We are given
\[
    \prod_a \theta_i = \frac{1}{n!} \epsilon^{ij\ldots l} \theta_i \theta_j
    \ldots \theta_l \,.
\]
We have used $a$ on the left side in order to distinguish from $i$. We will
need that distinction in a bit. Now we want to “divide” by the Levi-Civita
symbol~$\epsilon$. We do that in the following way: If we have an equation that
has free indices $ij \ldots l$ and we contract it with the present Levi-Civita
symbol, we have found the above equation “divided by $\epsilon$” such that it
is on the other side. We take $\epsilon$ to be normalized to $n!$. Then the
desired relation is
\[
    \epsilon_{ij \ldots l} \prod_a \theta_a = \theta_i \theta_j \ldots \theta_l
    \,.
\]
Multiplication and contraction with $\epsilon$ will give a factor of $n!$ on
the left side. Moving that to the right gives the $1/n!$ needed there.

Now we can rewrite the transformed product. This is
\begin{align*}
    \prod_a \theta_a'
    &= \frac{1}{n!} \epsilon^{ij \ldots l} \theta_i' \theta_j' \ldots \theta_l'
    \,.
    \intertext{%
        Then we can expand the $\theta'$ in terms of $\theta$. We obtain
    }
    &= \frac{1}{n!} \epsilon^{ij \ldots l} U_i^{i'} U_j^{j'} \ldots U_l^{l'}
    \theta_{i'} \theta_{j'} \ldots \theta_{l'} \,.
    \intertext{%
        Now we can use the relation shown above. This will transform the
        $\theta$ on the right side to
    }
    &= \frac{1}{n!} \epsilon^{ij \ldots l} U_i^{i'} U_j^{j'} \ldots U_l^{l'}
    \epsilon_{i'j' \ldots l'} \prod_a \theta_a \,.
    \intertext{%
        The fancy factor in front of the product symbol is just the determinant
        of the matrix $\mat U$. This mean that we have
    }
    &= \det(\mat U) \prod_a \theta_a \,.
\end{align*}
The change with an arbitrary linear transformation therefore changes the
product by its determinant.'

At this point the similarity to the differential forms which are also
antisymmetric is very apparent. A linear transformation introduces the
\emph{Jacobian} which is nothing else than this determinant of the linear
transformation matrix.

Again, if $\mat U \in \SU(n)$, then $\det(\mat U) = 1$ and the integral is
invariant under that transformation.

\subsection{Integral identities}

We have seen before that the integral “stamps out” the factor in front of the
Grassmann variable in question. This will simplify the first integral
significantly. A change of variables with an unitary transformation does not
change the integral. The matrices of interest to quantum theory can be
diagonalized by an unitary transformation. Therefore we can rewrite the given
integral\footnote{%
    The exterior derivate was missing in the volume elements. The integral
    would then be over a 0-form which I do not really know what that would be.
    I guess it would be zero as the result would be a $-1$-form which is not
    possible. Anyway, I just added the “d” as that is clear from the context
    \Smiley.
} as
\begin{align*}
    \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a} \exp(- \bar\theta_i B_{ij} \theta_j)
    &= \sbr{\int \prod_a \dif \bar\theta_a' \dif \theta_a'}
    \exp\del{- \sum _i \bar\theta_i' b_i \theta_i'}
    \,,
    \intertext{%
        where we name the eigenvalues of $\mat B$ just $b_i$. Now the integrals
        are actually independent. This only works because pairs of Grassmann
        numbers commute with other pairs. We can rewrite the integrals as
    }
    &= \prod_i \int \dif \bar\theta_i' \dif \theta_i'
    \exp\del{- \bar\theta_i' b_i \theta_i'}
    \,.
    \intertext{%
        Due to the antisymmetry of the variables we can write out all the
        summands of the exponential fully. That is a first, writing out a
        normal exponential is quite time consuming \Tongey. Anyway, we then have
    }
    &= \prod_i \int \dif \bar\theta_i' \dif \theta_i'
    \sbr{1 - \bar\theta_i' \theta_i' b_i}
    \,.
    \intertext{%
        The integration stamps out the $b_i$ and the remainder is
    }
    &= \prod_i b_i
    \intertext{%
        which is just
    }
    &= \det(\mat B) \,.
\end{align*}

The other one can be shown using the differentiation again. So we obtain the
desired bilinear using a partial integration. The bilinear is in the wrong
order in the exponential. Luckily we obtain another minus sign from the
exponential. Anticommuting the two Grassmann variables will remove the minus
sign and leave us with a nice expression without any signs.
\begin{align*}
    \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a} \theta_k \bar\theta_l
    \exp(- \bar\theta_i B_{ij} \theta_j)
    &= \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a} \pd{}{B_{kl}}
    \exp(- \bar\theta_i B_{ij} \theta_j)
    \intertext{%
        Assuming all behaves well we can pull out the derivative.
    }
    &= \pd{}{B_{kl}} \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a}
    \exp(- \bar\theta_i B_{ij} \theta_j)
    \intertext{%
        This integral was already computed and we insert the result.
    }
    &= \pd{}{B_{kl}} \det(\mat B)
    \intertext{%
        The determinant can be written as a lot of $B$s and Levi-Civita
        symbols.
    }
    &= \pd{}{B_{kl}} \frac{1}{n!}
    \epsilon_{ab \ldots d}
    B^a{}_{a'}
    B^b{}_{b'}
    \ldots
    B^d{}_{d'}
    \epsilon^{a'b' \ldots d'}
    \intertext{%
        The product rule (or Leibniz rule if we want to sound fancy) will
        create $n$ terms. They are all equal in the following sense: The matrix
        element differentiated will have an index pair of $i$ and $i'$. This
        has the same position in both Levi-Civita symbols. We can permute the
        indices on both symbols at the same time such that $i$ and $i'$ are the
        front element. Then we rename all the other dummy indices such that the
        terms are the same. Our combined terms are then
    }
    &= \frac{n}{n!}
    \epsilon_{kb \ldots d}
    B^b{}_{b'}
    \ldots
    B^d{}_{d'}
    \epsilon^{lb' \ldots d'} \,.
    \intertext{%
        According to \textcite[Fig.~13.7]{penrose-road_to_reality} the inverse
        of a matrix is very similar to that, except that we also need to divide
        by the $n!$ times determinant to get it right. Since we do not have
        that term here, we have to multiply with it.
    }
    &= \det(\mat B) (\mat B\inv)_{kl}
\end{align*}
That is the final result then.

\subsection{Generating functional}

The generating functional was introduced to allow the build-up of correlation
functions using functional derivatives. We had a homework where we worked with
those kind of integrals where a source term $J$ was added. At the time I did a
functional derivative with respect to the matrix elements. You wrote that we
are supposed to do that with respect to $J$. I do see why that is the case now.

This functional was not really derived, it was rather defined and shown that it
is quite useful. The same probably happens here. We propose the generating
function for the free Dirac theory to include some source terms. The
correlation functions can have $\psi$ and $\bar\psi$ terms in them, therefore
we need two source terms. Those source terms are taken to be $\eta$ and
$\bar\eta$. This has the nice side effect that the Lagrangian density will stay
real for every choice of complex $\eta$ and $\psi$.

Having that in mind, it is not a big leap to define\footnote{%
    I do not like to write $Z[\bar\eta, \eta]$ as that notation looks like $Z$
    being multiplied with the commutator of the two variables. Also I want to
    be able to distinguish $H(t - t_0)$ from $H[t - t_0]$, i.e.\ the
    Hamiltonian \emph{evaluated} at $t - t_0$ and the Hamiltonian
    \emph{multiplied} at $t - t_0$, both is a reasonable thing. I chose to
    represent this with different parentheses. The whole rationale is at
    \url{http://martin-ueding.de/en/physics/function-notation/index.html} I you
    want to know more about my idiosyncrasies \Winkey.
}
\[
    Z(\bar\eta, \eta) = \int \mathcal D \bar\psi \, \mathcal D \bar\psi
    \exp\del{\iup \int \dif^4 x \sbr{\bar\psi [\iup \slashed\partial - m]
    \psi + \bar\eta \psi + \bar\psi \eta}} \,.
\]
From here we have to show that it takes the form in the second line which
relies on this being a free theory where all the Gaussian integrals are only
quadratic in the exponent.

The integration by parts as done by \textcite[290]{Peskin/QFT/1995} is not
needed here, we have a first order differential operator $\hat O$ here such
that we have a bilinear of the form $\bar\psi \hat O \psi$ already. Therefore
we can directly strive to complete the “square”. Here it rather means to
complete the terms linear in the spinors to bilinears. The following
substitutions seem to work rather well:
\[
    \psi'(x) = \psi(x) - \iup \int \dif^4 y \, S_\text F(x - y) \eta(y)
    \eqnsep
    \bar\psi'(x) = \bar\psi(x) - \iup \int \dif^4 z \, S_\text F(x - z)
    \bar\eta(y) \,.
\]
The second is just the hermitian conjugate of the other one. Then we insert
that into the Lagrangian density. Writing out the whole thing here would just
kill the paper. The density is
\begin{align*}
    \mathcal L
    &= \bar\psi [\iup \slashed\partial - m] \psi
    + \bar\eta \psi + \bar\psi \eta \,.
    \intertext{%
        With the new fields it is
    }
    &= \sbr{\bar\psi'(x) + \iup \int \dif^4 z \, S_\text F(x - z)
    \bar\eta(z)}
    [\iup \slashed\partial - m]
    \sbr{\psi'(x) + \iup \int \dif^4 y \, S_\text F(x - y) \eta(y)}
    \\&\quad
    + \bar\eta 
    \sbr{\psi'(x) + \iup \int \dif^4 y \, S_\text F(x - y) \eta(y)}
    + \sbr{\bar\psi'(x) + \iup \int \dif^4 z \, S_\text F(x - z) \bar\eta(z)}
    \eta \,.
    \intertext{%
        This has to be factored out. Some of the terms will be old Lagrangian
        density with the primed spinors. We write this as $\mathcal L'$. The
        new terms are now
    }
    &= \mathcal L'
    - \int \dif^4 z \, S_\text F(x - z) \bar\eta(z)
    [\iup \slashed\partial - m]
    \int \dif^4 y \, S_\text F(x - y) \eta(y)
    \\&\quad
    +
    \iup \bar\psi'(x)
    [\iup \slashed\partial - m]
    \int \dif^4 y \, S_\text F(x - y) \eta(y)
    +
    \iup \int \dif^4 z \, S_\text F(x - z) \bar\eta(z)
    [\iup \slashed\partial - m]
    \psi'(x)
    \\&\quad
    + \iup \bar\eta \int \dif^4 y \, S_\text F(x - y) \eta(y)
    + \iup \int \dif^4 z \, S_\text F(x - z) \bar\eta(z) \eta \,.
    \intertext{%
        In the book they stress that the propagator is the Green's function to
        the kinetic operator. Therefore it is the “operator inverse” that will
        give a Dirac $\delta$-distribution. That is then integrated over.
        Perhaps it gives an imaginary unit or some other phase factor. Either
        way, the first three summands simplify this way. We add the explicit
        function argument to the remaining factors, this is just $x$ by
        default.
    }
    &= \mathcal L'
    - \int \dif^4 y \, \bar\eta(x) S_\text F(x - y) \eta(y)
    + \iup \bar\psi'(x) \eta(x)
    + \iup \bar\eta(x) \psi'(x)
    \\&\quad
    + \iup \int \dif^4 y \, \bar\eta(x) S_\text F(x - y) \eta(y)
    + \iup \int \dif^4 z \, \bar\eta(z) S_\text F(x - z) \eta(x) \,.
\end{align*}
The last two terms are equal to the first one except for the sign. It
would be very handy if we had a phase factor from the Green's function property
that we have just used. Then two of the three terms would cancel and one would
be left with just a factor of one imaginary unit. Also the $\bar\psi' \eta$ and
the hermitian conjugate term would cancel with the source terms present in
$\mathcal L'$. Then the final thing would be
\[
    \mathcal L = \mathcal L_0 + 
    + \iup \int \dif^4 z \, \bar\eta(z) S_\text F(x - z) \eta(x) \,.
\]
Plugging this into the exponential function where we have $\exp(- \iup \int
\dif^4 x \mathcal L)$, we would get the desired $Z_0$ times the exponential
with a negative sign in it.

I am sorry I cannot look closer into that right now. Please tell me in the
tutorial (or on this sheet) what the phase factor is if I plug in the Green's
function into the equation itself.

\end{document}

% vim: spell spelllang=en tw=79
