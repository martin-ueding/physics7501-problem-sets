\documentclass[11pt, english, fleqn, DIV=15, headinclude]{scrartcl}

\usepackage[bibatend]{../header}
\usepackage{../my-boxes}

\usepackage{lastpage}
\usepackage{multicol}
\usepackage{simplewick}
\usepackage{multicol}
\usepackage{slashed}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{tikzsymbols}

\newcommand\timeorder{\mathscr T}
\newcommand\normorder{\mathscr N}
\newcommand\eye{\mat 1_4}
\newcommand\fourslash[1]{\slashed{\four{#1}}}
\newcommand\T{\mathrm T}

\hypersetup{
    pdftitle=
}

\graphicspath{{build/}}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{5}

\subject{physics7501 -- Advanced Quantum Field Theory}
\ihead{physics7501 -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\newcommand\thegroup{Tutor: Thorsten Schimannek}

\publishers{\thegroup}
\ofoot{\thegroup}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
}
\ifoot{Martin Ueding}

\ohead{\rightmark}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        Problem & Achieved points & Possible points \\
        \midrule
        \nameref{homework:1} & & \punkte{15} \\
        \midrule
        Total & & \arabic{totalpoints}
    \end{tabular}
\end{center}

\vspace{3ex}

\begin{center}
    \begin{small}
        This document consists of \pageref{LastPage} pages.
    \end{small}
\end{center}

\section{Grassmann numbers and the fermionic path integral}
\label{homework:1}

\subsection{Taylor series and shift}

The Grassmann numbers themselves are the generators of the algebra. It probably
makes sense to add 1 to the generators also in order to have $\C$  be part of
the algebra as well. We will call that set $\setname G$.\footnote{%
    The notation $\mathbb C$ and $\mathbb G$ might be more common for fields. I like to go
    with the ISO 80000-2 standard (which allows both variants) and also take
    the heritage of those letters (like $\mathbb C$) into account. In \LaTeX\
    they are called “blackboard bold” which means this way of writing letters
    is the attempt to write bold on the blackboard. Since we have more powerful
    typesetting in \LaTeX\ than on the board, I will sure make use of them and
    display the letters in bold. Same goes for underline for emphasis, which is
    a no-go in typed documents. Perhaps it is not the best idea to deviate too
    much from common notation even though its usage in a powerful typesetting
    system feels rather backwards.
}
The function $f$ then is
\[
    f \colon \setname G \to \setname G \,.
\]
Since the Grassmann numbers anticommute they square to zero.\footnote{%
    On the Wikipedia article about Grassmann numbers they are called “non-zero
    square roots of zero” which I find a quite intriguing thought. Almost ten
    years ago the concept of $\sqrt{-1}$ has baffled me, now this is the next
    step. This time I know that field axioms are not god-given and therefore
    algebra structures can be defined like one wishes to furnish those axioms.
    Still, it is an interesting way of stating the squaring to zero feature.
}
This also means that a function of a single Grassmann variable cannot have
arbitrary form. Analytic functions can always be written as a power series,
functions can at most be linear in Grassmann variables which really limits
their complexity. Any function can be written as
\[
    f(\theta) = A + B \theta \,,
\]
where we use the same notation as \textcite[299]{Peskin/QFT/1995}. Higher order
terms in $\theta$ vanish directly.

\paragraph{Taylor series}

The Taylor series of such a function in $\theta$ just has two terms as higher
powers of $\theta$ will vanish. Also higher derivatives in $\theta$ will also
vanish when one looks at the general form motivated above.

\paragraph{Integral}

The integration must be invariant under shifts in the integration variable. We
have
\begin{align*}
    \int \dif \theta \, f(\theta)
    &= \int \dif \theta \, [A + B \theta]
    \intertext{%
        as the general form. Now shifting $\theta$ by $\eta$ does not change
        $\dif \theta$ and we have
    }
    &= \int \dif \theta \, [A + B [\theta + \eta]]
    \intertext{%
        which we can expand to yield
    }
    &= \int \dif \theta \, [A + B \theta + B \eta] \,.
    \intertext{%
        It is best to regroup the terms such that the constant and linear terms
        clearly pop out:
    }
    &= \int \dif \theta \, [\underbrace{A + B \eta}_{A'} + B \theta] \,.
\end{align*}
The result of the integration must not be changed when $A \to A'$ it performed.
Therefore the integral must not depend on $A$ as it is a linear function and
has to be rather simple. The only dependence can be $B$ then. A definition that
fulfils this is
\[
    \int \dif \theta \, [A + B\theta] = k B \,.
\]

Could this $k$ be a Grassmann number, i.e.\ $k \in \setname G$? $B$ is
definitely a regular complex number, so the product would not directly
vanish. However, the integral would then be a $\setname G$-linear function
which does not play well with integrations in $\C$. It therefore makes sense to
define $k \in \C$ and therefore $kB = c$ is a regular complex number.

\paragraph{Integration and Differentiation}

On the problem set it is noted that integration and differentiation are
equivalent. Since there are only two possible monoms of Grassmann variables
($A$ and $B \theta$), this is easy to check. The integral and derivative with
respect to $\theta$ of the first term is zero. The integral and derivative of
the second is just $B$. Setting $k = 1$ makes this truly the same thing which
is probably very handy along the road.

\subsection{Unitary transformation}

The unitary transformation is probably a \emph{special} unitary transformation.
In the case of a general unitary transformation the determinant might be a
complex phase factor which does change the integrals unless complex conjugated
counter-terms are involved as well. So $U$ is taken from $\SU(n)$ and $\mathrm
U(1)$ is excluded here? Since the problem talks about matrices, $\SU(n)$ would
not really be a problem. So this just applies when there are multiple fields?
\Textcite[301]{Peskin/QFT/1995} show that the integral with $\dif \theta^* \dif
\theta$ is invariant under $\mathrm U(n)$ but integrals with $\dif \theta$
depend on $\det(U)$ which might have a phase factor.

We will now show that the transformation introduces a factor on $\det(U)$. The
invariance has to be evaluated with respect of that determinant then which is
unity for the special transformations.

\begin{table}
    \begin{question}
        $\mathrm U(1)$ only has one generator, 1. Therefore the group $\SU(1) =
        \{ 1 \}$ is a quite trivial group. The groups $\SU(n)$ all have $n^2-1$
        generators and $\mathrm U(n)$ should have $n^2$ generators. Is it in
        general that
        \[
            \mathrm U(n) \simeq \mathrm U(1) \times \SU(n)
        \]
        holds?
    \end{question}
\end{table}

First we show a useful identity. We are given
\[
    \prod_a \theta_i = \frac{1}{n!} \epsilon^{ij\ldots l} \theta_i \theta_j
    \ldots \theta_l \,.
\]
We have used $a$ on the left side in order to distinguish from $i$. We will
need that distinction in a bit. Now we want to “divide” by the Levi-Civita
symbol~$\epsilon$. We do that in the following way: If we have an equation that
has free indices $ij \ldots l$ and we contract it with the present Levi-Civita
symbol, we have found the above equation “divided by $\epsilon$” such that it
is on the other side. We take $\epsilon$ to be normalized to $n!$. Then the
desired relation is
\[
    \epsilon_{ij \ldots l} \prod_a \theta_a = \theta_i \theta_j \ldots \theta_l
    \,.
\]
Multiplication and contraction with $\epsilon$ will give a factor of $n!$ on
the left side. Moving that to the right gives the $1/n!$ needed there.

Now we can rewrite the transformed product. This is
\begin{align*}
    \prod_a \theta_a'
    &= \frac{1}{n!} \epsilon^{ij \ldots l} \theta_i' \theta_j' \ldots \theta_l'
    \,.
    \intertext{%
        Then we can expand the $\theta'$ in terms of $\theta$. We obtain
    }
    &= \frac{1}{n!} \epsilon^{ij \ldots l} U_i^{i'} U_j^{j'} \ldots U_l^{l'}
    \theta_{i'} \theta_{j'} \ldots \theta_{l'} \,.
    \intertext{%
        Now we can use the relation shown above. This will transform the
        $\theta$ on the right side to
    }
    &= \frac{1}{n!} \epsilon^{ij \ldots l} U_i^{i'} U_j^{j'} \ldots U_l^{l'}
    \epsilon_{i'j' \ldots l'} \prod_a \theta_a \,.
    \intertext{%
        The fancy factor in front of the product symbol is just the determinant
        of the matrix $\mat U$. This mean that we have
    }
    &= \det(\mat U) \prod_a \theta_a \,.
\end{align*}
The change with an arbitrary linear transformation therefore changes the
product by its determinant.'

At this point the similarity to the differential forms which are also
antisymmetric is very apparent. A linear transformation introduces the
\emph{Jacobian} which is nothing else than this determinant of the linear
transformation matrix.

Again, if $\mat U \in \SU(n)$, then $\det(\mat U) = 1$ and the integral is
invariant under that transformation.

\subsection{Integral identities}

We have seen before that the integral “stamps out” the factor in front of the
Grassmann variable in question. This will simplify the first integral
significantly. A change of variables with an unitary transformation does not
change the integral. The matrices of interest to quantum theory can be
diagonalized by an unitary transformation. Therefore we can rewrite the given
integral\footnote{%
    The exterior derivate was missing in the volume elements. The integral
    would then be over a 0-form which I do not really know what that would be.
    I guess it would be zero as the result would be a $-1$-form which is not
    possible. Anyway, I just added the “d” as that is clear from the context
    \Smiley.
} as
\begin{align*}
    \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a} \exp(- \bar\theta_i B_{ij} \theta_j)
    &= \sbr{\int \prod_a \dif \bar\theta_a' \dif \theta_a'}
    \exp\del{- \sum _i \bar\theta_i' b_i \theta_i'}
    \,,
    \intertext{%
        where we name the eigenvalues of $\mat B$ just $b_i$. Now the integrals
        are actually independent. This only works because pairs of Grassmann
        numbers commute with other pairs. We can rewrite the integrals as
    }
    &= \prod_i \int \dif \bar\theta_i' \dif \theta_i'
    \exp\del{- \bar\theta_i' b_i \theta_i'}
    \,.
    \intertext{%
        Due to the antisymmetry of the variables we can write out all the
        summands of the exponential fully. That is a first, writing out a
        normal exponential is quite time consuming \Tongey. Anyway, we then have
    }
    &= \prod_i \int \dif \bar\theta_i' \dif \theta_i'
    \sbr{1 - \bar\theta_i' \theta_i' b_i}
    \,.
    \intertext{%
        The integration stamps out the $b_i$ and the remainder is
    }
    &= \prod_i b_i
    \intertext{%
        which is just
    }
    &= \det(\mat B) \,.
\end{align*}

The other one can be shown using the differentiation again. So we obtain the
desired bilinear using a partial integration. The bilinear is in the wrong
order in the exponential. Luckily we obtain another minus sign from the
exponential. Anticommuting the two Grassmann variables will remove the minus
sign and leave us with a nice expression without any signs.
\begin{align*}
    \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a} \theta_k \bar\theta_l
    \exp(- \bar\theta_i B_{ij} \theta_j)
    &= \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a} \pd{}{B_{kl}}
    \exp(- \bar\theta_i B_{ij} \theta_j)
    \intertext{%
        Assuming all behaves well we can pull out the derivative.
    }
    &= \pd{}{B_{kl}} \sbr{\int \prod_a \dif \bar\theta_a \dif \theta_a}
    \exp(- \bar\theta_i B_{ij} \theta_j)
    \intertext{%
        This integral was already computed and we insert the result.
    }
    &= \pd{}{B_{kl}} \det(\mat B)
    \intertext{%
        The determinant can be written as a lot of $B$s and Levi-Civita
        symbols.
    }
    &= \pd{}{B_{kl}} \frac{1}{n!}
    \epsilon_{ab \ldots d}
    B^a{}_{a'}
    B^b{}_{b'}
    \ldots
    B^d{}_{d'}
    \epsilon^{a'b' \ldots d'}
    \intertext{%
        The product rule (or Leibniz rule if we want to sound fancy) will
        create $n$ terms. They are all equal in the following sense: The matrix
        element differentiated will have an index pair of $i$ and $i'$. This
        has the same position in both Levi-Civita symbols. We can permute the
        indices on both symbols at the same time such that $i$ and $i'$ are the
        front element. Then we rename all the other dummy indices such that the
        terms are the same. Our combined terms are then
    }
    &= \frac{n}{n!}
    \epsilon_{kb \ldots d}
    B^b{}_{b'}
    \ldots
    B^d{}_{d'}
    \epsilon^{lb' \ldots d'} \,.
    \intertext{%
        According to \textcite[Fig.~13.7]{penrose-road_to_reality} the inverse
        of a matrix is very similar to that, except that we also need to divide
        by the $n!$ times determinant to get it right. Since we do not have
        that term here, we have to multiply with it.
    }
    &= \det(\mat B) (\mat B\inv)_{kl}
\end{align*}
That is the final result then.

\end{document}

% vim: spell spelllang=en tw=79
