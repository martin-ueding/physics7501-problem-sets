\documentclass[11pt, english, fleqn, DIV=15, headinclude]{scrartcl}

\usepackage[bibatend]{../header}
\usepackage{../my-boxes}

\usepackage{lastpage}
\usepackage{multicol}
\usepackage{simplewick}
\usepackage{slashed}
\usepackage{subcaption}
\usepackage{cancel}

\newcommand\timeorder{\mathscr T}
\newcommand\normorder{\mathscr N}
\newcommand\eye{\mat 1_4}
\newcommand\fourslash[1]{\slashed{\four{#1}}}
\newcommand\T{\mathrm T}

\hypersetup{
    pdftitle=
}

\graphicspath{{build/}}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{3}

\subject{physics7501 -- Advanced Quantum Field Theory}
\ihead{physics7501 -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\newcommand\thegroup{Tutor: Thorsten Schimmanek}

\publishers{\thegroup}
\ofoot{\thegroup}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
}
\ifoot{Martin Ueding}

\ohead{\rightmark}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        Problem & Achieved points & Possible points \\
        \midrule
        \nameref{homework:1} & & \punkte{10} \\
        \nameref{homework:2} & & \punkte{10} \\
        \midrule
        Total & & \arabic{totalpoints}
    \end{tabular}
\end{center}

\vspace{3ex}

\begin{center}
    \begin{small}
        This document consists of \pageref{LastPage} pages.
    \end{small}
\end{center}

\section{The path integral in quantum mechanics}
\label{homework:1}

\subsection{Propagator}

\newcommand\xF{x_\mathrm f}
\newcommand\xI{x_\mathrm i}

I have to show that
\begin{align*}
    G(t, \xF, \xI)
    &= \int \dif x \, G(t - t', \xF, x) \, G(g', x, \xI) \,.
\end{align*}

This is straightforward. First we replace $G$ with the matrix element.
\begin{align*}
    G(t, \xF, \xI)
    &= \bra\xF \exp(- \iup t H) \ket\xI
    \intertext{%
        The exponential can be taken apart. The Hamiltonian commutes with
        itself and therefore the exponentials work like with c-numbers.
    }
    &= \bra\xF \exp(- \iup [t-t'] H) \exp(- \iup t' H) \ket\xI
    \intertext{%
        Then we insert a complete set of position eigenstates in between the
        exponentials.
    }
    &= \int \dif x \, \bra\xF \exp(- \iup [t-t'] H) \ket x \bra x \exp(- \iup t' H) \ket\xI
    \intertext{%
        Those matrix elements are propagators again. We arrive at
    }
    &= \int \dif x \, G(t - t', \xF, x) \, G(g', x, \xI) \,,
\end{align*}
which is the desired expression.

Next we check that adding a Heaviside step function makes it a retarded
Greens-function.
\begin{align*}
    [\iup \partial_0 - H] \, G_\mathrm r(t, \xF, \xI)
    &= [\iup \partial_0 - H] \, \Theta(t) \, G(t, \xF, \xI)
    \intertext{%
        We insert the explicit form of the propagator.
    }
    &= [\iup \partial_0 - H] \, \Theta(t) \, \bra\xF \exp(- \iup t H) \ket\xI
    \intertext{%
        The time derivative will give me two terms.
    }
    &= \iup \, \delta(t) \, \bra\xF \exp(- \iup t H) \ket\xI
    - \Theta(t) \, \bra\xF H \exp(- \iup t H) \ket\xI
    \\&\quad
    - H \Theta(t) \, \bra\xF \exp(- \iup t H) \ket\xI
    \intertext{%
        The first term will only contribute if $t = 0$. Since the exponential
        is smooth in $t$, one can just set $t = 0$ there. The other two terms
        should cancel. Letting the Hamiltonian act on $\bra \xF$ in both cases
        gives the same energy in both cases. The remainder of the first term is
        then the following:
    }
    &= \iup \, \delta(t) \, \braket{\xF|\xI} \,.
    \intertext{%
        And this simplifies to the desired right side of the equation:
    }
    &= \iup \, \delta(t) \, \delta(\xF - \xI) \,.
\end{align*}

\subsection{Split}

We have the propagator
\begin{align*}
    G(t, \xF, \xI)
    &= \bra\xF \exp(- \iup t H) \ket\xI
    \intertext{%
        where we can introduce $\epsilon$ and $N$.
    }
    &= \bra\xF \exp(- \iup \epsilon N H) \ket\xI \\
    \intertext{%
        Then we write the product $\epsilon N$ as a sum
    }
    &= \bra\xF \exp\del{- \sum_{j=1}^N \iup \epsilon H} \ket\xI \\
    \intertext{%
        which we pull out of the exponential to become a product. This only
        works since the Hamiltonian commutes with itself.
    }
    &= \bra\xF \sbr{\prod_{j=1}^N \exp\del{- \iup \epsilon H}} \ket\xI \\
    \intertext{%
        We pull one exponential out of the product.
    }
    &= \bra\xF \exp\del{- \iup \epsilon H} \sbr{\prod_{j=1}^{N-1} \exp\del{-
    \iup \epsilon H}} \ket\xI \\
    \intertext{%
        And then we insert complete sets of position eigenstates in between.
    }
    &= \bra\xF \exp\del{- \iup \epsilon H} \sbr{\prod_{j=1}^{N-1} \int \dif x_j
    \ket{x_j} \bra{x_j} \exp\del{- \iup \epsilon H}} \ket\xI
\end{align*}
A little massaging will extract the $N-1$ integrals and the $N$ exponentials
sandwiched between position states into a form like the one given on the
problem set.

\subsection{Split of Hamiltonian}

We now look at the propagator for an infinitesimal time step.
\begin{align*}
    G(\epsilon, x_k, x_{k-1})
    &= \bra{x_k} \exp(-\iup \epsilon H) \ket{x_{k-1}}
    \intertext{%
        We insert the explicit Hamiltonian.
    }
    &= \bra{x_k} \exp\del{-\iup \epsilon \sbr{\hat T + \hat V}} \ket{x_{k-1}}
    \intertext{%
        The Baker-Campbell-Hausdorff formula says that we can split the
        exponential into two factors if we include a third exponential which
        starts with the commutator of the two parts. Since both $\hat T$ and
        $\hat V$ are premultiplied with $\epsilon$, the commutator will be
        order $\epsilon^2$ and we will therefore just drop that term.
    }
    &= \bra{x_k} \exp\del{-\iup \epsilon \hat T} \exp\del{-\iup \epsilon \hat V} \ket{x_{k-1}}
    \intertext{%
        We evaluate the potential on the in-state. After that it is just a
        c-number and we move it to the end of the expression.
    }
    &= \bra{x_k} \exp\del{-\iup \epsilon \hat T}\ket{x_{k-1}}
    \exp\del{-\iup \epsilon V(x_{k-1})}
    \intertext{%
        In order to evaluate the kinetic energy we need momentum eigenstates.
        We can have those, we just have to take all of them at once.
    }
    &= \int \dif p \, \bra{x_k} \exp\del{-\iup \epsilon \hat T} \ket{p}
    \braket{p|x_{k-1}}
    \exp\del{-\iup \epsilon V(x_{k-1})}
    \intertext{%
        The action of the kinetic energy is now easy. After the evaluation of
        the operator it is just another c-number.
    }
    &= \int \dif p \, \braket{x_k | p}
    \braket{p|x_{k-1}}
    \exp\del{-\iup \epsilon \frac{p^2}{2m}}
    \exp\del{-\iup \epsilon V(x_{k-1})}
    \intertext{%
        Now we have to insert the explicit overlap.
    }
    &= \int \frac{\dif p}{2\piup} \,
    \frac{1}{2\piup} \exp\del{\iup [x_k - x_{x-1}] p}
    \exp\del{-\iup \epsilon \frac{p^2}{2m}}
    \exp\del{-\iup \epsilon V(x_{k-1})}
    \intertext{%
        We combine the $p$-dependent terms after the integral and pull the
        potential out up front.
    }
    &= \exp\del{-\iup \epsilon V(x_{k-1})}
    \int \frac{\dif p}{[2\piup]^2} \,
    \exp\del{-\iup \epsilon \frac{p^2}{2m} + \iup [x_k - x_{x-1}] p}
    \intertext{%
        Next up is completion of the square in $p$ in the exponential.
    }
    &= \exp\del{-\iup \epsilon V(x_{k-1})}
    \int \frac{\dif p}{[2\piup]^2} \,
    \\&\quad \times
    \exp\del{-\sbr{\sqrt{\frac{\iup \epsilon}{2m}} p - \sqrt{\frac{\iup
    m}{2\epsilon}} [x_k - x_{x-1}]}^2 + \frac{\iup m}{2\epsilon} [x_k -
    x_{x-1}]^2}
    \intertext{%
        The last part of the exponential can be pulled in front of the
        integral.
    }
    &= \exp\del{-\iup \epsilon V(x_{k-1})}
    \exp\del{\frac{\iup m}{2\epsilon} [x_k - x_{x-1}]^2}
    \\&\quad \times
    \int \frac{\dif p}{[2\piup]^2} \,
    \exp\del{-\sbr{\sqrt{\frac{\iup \epsilon}{2m}} p - \sqrt{\frac{\iup
    m}{2\epsilon}} [x_k - x_{x-1}]}^2}
    \intertext{%
        Due to the infinite bounds of the integration we can just shift the
        integration variable $p$ by a fixed amount and then have the simpler
        integral
    }
    &= \exp\del{-\iup \epsilon V(x_{k-1})}
    \exp\del{\frac{\iup m}{2\epsilon} [x_k - x_{x-1}]^2}
    \int \frac{\dif p}{[2\piup]^2} \,
    \exp\del{-\frac{\iup \epsilon}{2m} p^2} \,.
    \intertext{%
        This type of integral will be derived in the second part of the
        problem. It was also derived in the “Mathematischer Vorkurs” in 2011
        and every other lecture, especially AQT. So that is just $\sqrt{\pi/a}$
        where $a$ is the factor such that the integrand is $\exp(-a p^2)$. We
        then have
    }
    &=
    \frac{1}{[2\piup]^2}
    \sqrt{\frac{2\piup m}{\iup \epsilon}}
    \exp\del{\frac{\iup m}{2\epsilon} [x_k - x_{x-1}]^2 -\iup \epsilon V(x_{k-1})}
    \,.
    \intertext{%
        For some reason there is a factor $2 \piup$ too much here. We will
        include one of them in the square root. We will also factor out $\iup
        \epsilon$ in the exponential.
    }
    &= \frac{1}{2\piup} \sqrt{\frac{m}{2 \piup \iup \epsilon}}
    \exp\del{\iup \epsilon \sbr{\frac{m}{2} \frac{[x_k -
    x_{x-1}]^2}{\epsilon^2} - V(x_{k-1})}}
\end{align*}
Except for the additional factor of $2 \piup$ this is the desired result.

\subsection{Limit}

This limit makes the integral a functional integral which integrates over all
functions of $x$ which satisfy the boundary conditions. One combines an
infinite amount of propagators and takes the limit $\epsilon \to 0$ such that
their combination becomes in integral. All the intermediate positions have to
be integrated over. The result then is the Feynman path integral.

\begin{question}
    There is a fundamental problem with this. The space of all functions that
    have $x(0) = \xI$ and $x(t) = \xF$ as boundary conditions is uncountable
    infinite, it has cardinality $c = 2^{\aleph_0}$. The limit $N \to \infty$
    can only make $N \to \aleph_0$ which is smaller in an appropriate sense as
    we have $\aleph_0/c \to 0$. Therefore we are ignoring virtually all
    functions that contribute! Mathematicians sometimes ask me how I sleep at
    night and the only things I can offer are ignorance and the number of
    digits in the electron's magnetic moment. Is there something more to this?
\end{question}


This formalism with finite $N$ is very useful for lattice computation of
quantum systems \parencite{Creutz/Statistical_Approach_QM}. One has to do this
in imaginary time such that the quantum mechanical system evolving in time as
mediated by the Lagrangian becomes a crystal where the neighboring sites are
coupled by the Lagrangian. That is what I did in my bachelor's thesis with the
harmonic oscillator \parencite{Ueding/Bachelorarbeit}.

\section{Multi-dimensional gaussian integration}
\label{homework:2}

\subsection{First integral}

We did pretty much the same thing on an AQT sheet\footnote{It was the third
problem set, you can find all my AQT solutions at\\
\url{http://martin-ueding.de/de/studies/msc_physics/physics606/index.html}.}
already, so we will shamelessly copy the year-old result.

We need this integral as a preliminary:
\[
    \int_0^\infty \dif x \, x \exp(-ax^2) \,.
\]
We do this by using substitution with
\[
    z := x^2
    \eqnsep
    \dif z = 2 x \, \dif x \,.
\]
Using that, we yield
\[
    \int \dif x \, x \exp(-ax^2) = \frac{1}{2} \int \dif z \, \exp(-az) = -
    \frac{1}{2a} \exp(-az) \,.
\]
Evaluating this at the boundary $[0, \infty)$ we obtain the result
\[
    \frac{1}{2a}.
\]

If the real part of $a$ is negative, the integrand is not bounded any more and
the function is not integrable any more. The result
\[
    \frac{1}{2a}
\]
would turn negative for negative $\Re a$. However, the integrand itself would
still be positive semi-definite. Therefore, this cannot be right. Unless one
goes into the realm of complex magic where
\[
    \sum_{i = 0}^\infty 2^{2i} = - \frac{1}{3}
\]
can make some sense \parencite[78]{penrose-road_to_reality}.

The actual integral to compute needs to be squared first to use the polar
coordinates trick.
\begin{align*}
    |I_0(a)|^2
    &=
    \int_{_\infty}^\infty \dif x \exp(-ax^2)
    \int_{_\infty}^\infty \dif y \exp(-ay^2) \\
    &=
    \int_{_\infty}^\infty \dif x \int_{_\infty}^\infty \dif y \exp(-a[x^2+y^2]) \\
    \intertext{%
        Now $x^2 + y^2 = r^2$ in polar coordinates. The measure of the
        integration, which can be derived from Gram's determinant of the
        metric tensor, is $r \dif r \dif \phi$. One could also try to sound
        fancy by calling this the pullback/pushforward of the volume form.
    }
    &= \int_0^\infty r \dif r \int_0^{2\piup} \dif \phi \exp(-ar^2) \\
    \intertext{%
        Since the integrand does not depend on $\phi$, we will get a simple
        scalar factor:
    }
    &= 2 \piup \int_0^\infty r \dif r \exp(-ar^2) \,. \\
    \intertext{%
        Using the previously derived fact that this integral is $1/2a$ we can
        write down the result:
    }
    &= \frac{\piup}a \,.
\end{align*}
That was the square of the integral to be calculated, so the integral itself is
\[
    \sqrt{\frac\piup a} \,.
\]
In our case we have $a = 1/2$ and therefore the end result is $\sqrt{2 \piup}$.

\subsection{Second integral}

Now there is a matrix product product in the exponential. We introduce an
orthogonal transformation $\mat O$ such that $\mat O\inv = \mat O^\T$. Since
the determinant is multiplicative and is invariant under the transpose,
$\det(\mat O) = \pm 1$.
We start by introducing the unit matrix in form of the transformation.
\begin{align*}
    \int \dif x^d \, \exp\del{- \frac12 \vec x^\T \mat A \vec x}
    &= \int \dif x^d \,
    \exp\del{- \frac12 \vec x^\T \mat O^\T \mat O \mat A \mat O^\T \mat O \vec x}
    \intertext{%
        The matrix $\mat A$ will be diagonalized by this basis transformation
        and we get the eigenvalue matrix $\mat\Lambda$.
    }
    &= \int \dif x^d \,
    \exp\del{- \frac12 \vec x^\T \mat O^\T \mat \Lambda \mat O \vec x}
    \intertext{%
        The transposed vector can be grouped with the transformation.
    }
    &= \int \dif x^d \,
    \exp\del{- \frac12 [\mat O \vec x]^\T \mat \Lambda \mat O \vec x}
    \intertext{%
        The integrand can be simplified with a substitution $\vec y := \mat O
        \vec x$ and therefore $\vec x = \mat O^\T \vec y$. The exterior
        derivative gives $\dif x^i = O^{\T \, i}{}_j \dif y^j$. The $d$-form
        $\dif x^d$ can be written as $\epsilon_{i \ldots k} \dif x^i \wedge
        \ldots \wedge \dif x^k$. Those get replaced by $O^{\T \, i}{}_j \dif
        y^j$. Then those $\dif y^j$ are wedged together and are also
        antisymmetric. In the end one has $\epsilon_{i \ldots k} O^{\T \,
        i}{}_m \ldots O^{\T \, i}{}_o \epsilon^{m \ldots o}$ which is just the
        determinant of $\mat O$. And that is $\pm 1$ where one might even chose
        that is has to have $+1$ and move the $-1$ into the eigenvalue. Then
        the expression simplifies to
    }
    &= \int \dif y^d \,
    \exp\del{- \frac12 \vec y^\T \mat \Lambda \vec y} \,.
    \intertext{%
        The matrix $\mat \Lambda$ is diagonal. Therefore one can write the
        matrix product simpler as
    }
    &= \int \dif y^d \,
    \exp\del{- \frac12 \sum_i \lambda_i [y^i]^2} \,.
    \intertext{%
        The sum can be pulled out of the exponential.
    }
    &= \prod_i \int \dif y^i \,
    \exp\del{- \frac12 \lambda_i [y^i]^2}
    \intertext{%
        Those integrals will just give $\sqrt{2\piup/\lambda_i}$.
    }
    &= \prod_i \sqrt{\frac{2\piup}{\lambda_i}}
    \intertext{%
        The product of the eigenvalues is the determinant of the matrix in
        question.
    }
    &= \sqrt{\frac{[2\piup]^2}{\det(\mat A)}}
\end{align*}
And that is the desired result.

\subsection{Integral with scalar product}

The basic idea is the same here. We insert pairs of transformations everywhere.
\begin{align*}
    \int \dif x^d \, \exp\del{- \frac12 \vec x^\T \mat A \vec x + \vec J^\T
    \vec x}
    &= \int \dif x^d \,
    \exp\del{- \frac12 \vec x^\T \mat O^\T \mat O \mat A \mat O^\T \mat O \vec x + \vec J^\T
    \mat O^\T \mat O \vec x} \\
    \intertext{%
        We define $\vec K := \mat O \vec x$ analogously to $\vec y$.
    }
    &= \int \dif y^d \,
    \exp\del{- \frac12 \vec y^\T \mat \Lambda \vec y + \vec K^\T
    \vec y} \\
    \intertext{%
        Then we can write the matrix multiplications on the exponential as a
        sum within or a product outside of the exponential.
    }
    &= \prod_i \int \dif y^i \,
    \exp\del{- \frac12 \lambda_i [y^i]^2 + K^i y^i}
    \intertext{%
        We can complete the square in $y^i$.
    }
    &= \prod_i \int \dif y^i \,
    \exp\del{- \sbr{\sqrt{\frac{\lambda_i}{2}} y^i - \frac{K^i}{\sqrt{2
    \lambda_i}}}^2 - \frac{[K^i]^2}{2 \lambda_i}}
    \intertext{%
        Then we shift the integration variable again and have
    }
    &= \prod_i \int \dif y^i \,
    \exp\del{- \frac{\lambda_i}{2} [y^i]^2 + \frac{[K^i]^2}{2 \lambda_i}} \,.
    \intertext{%
        We pull out the constant part of the exponential up front,
    }
    &= \prod_i \exp\del{\frac{[K^i]^2}{2 \lambda_i}} \int \dif y^i \,
    \exp\del{- \frac{\lambda_i}{2} [y^i]^2} \,,
    \intertext{%
        and then apply the previous result.
    }
    &= \exp\del{\frac 12 \sum_i \frac{[K^i]^2}{\lambda_i}}
    \sqrt{\frac{[2 \piup]^N}{\det(\mat A)}}
    \intertext{%
        The first exponential now contains $\vec K$ which is $\mat O \vec J$
        twice as well as the inverse eigenvalues. Therefore this whole
        expression is like $\vec J^\T \mat O^T \mat \lambda\inv \mat O \vec J$.
        The middle part is just $\mat A\inv$ and therefore the final result is
        the desired one,
    }
    &= \exp\del{\frac 12 \vec J^\T \mat A\inv \vec J}
    \sqrt{\frac{[2 \piup]^N}{\det(\mat A)}}
\end{align*}

\end{document}

% vim: spell spelllang=en tw=79
